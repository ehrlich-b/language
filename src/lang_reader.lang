// src/lang_reader.lang - Lang syntax as a reader
//
// This defines the "lang" reader - the syntax parser for .lang files.
// It wraps lexer.lang, parser.lang, and ast_emit.lang into a reader
// that transforms lang source code into S-expression AST.
//
// Smart about context:
//   #lang{ 1 + 1 }  → (binop + (number 1) (number 1))  (expression)
//   #lang{ func f() i64 { return 1; } }  → (program (func ...))  (declaration)
//
// This makes lang's syntax a plugin - just another reader that emits AST.

include "std/core.lang"
include "src/limits.lang"
include "src/lexer.lang"
include "src/parser.lang"
include "src/ast_emit.lang"

// Check if token type indicates a declaration
func is_decl_token(t i64) bool {
    return t == TOKEN_FUNC || t == TOKEN_VAR || t == TOKEN_STRUCT ||
           t == TOKEN_ENUM || t == TOKEN_EFFECT || t == TOKEN_MACRO ||
           t == TOKEN_READER || t == TOKEN_INCLUDE || t == TOKEN_IMPORT;
}

// The lang reader: parses lang syntax, emits S-expr AST
// Smart: detects whether input is expression or declarations
reader lang(text *u8) *u8 {
    // Tokenize the input
    parser_tokenize(text);

    // Check for lexer errors
    var i i64 = 0;
    while i < parser_token_count() {
        var tok *u8 = get_token(i);
        if tok_type(tok) == TOKEN_ERROR {
            eprint("lang reader error: ");
            eprintln(tok_lexeme(tok));
            return nil;
        }
        i = i + 1;
    }

    // Check first token to determine parse mode
    var first_type i64 = TOKEN_EOF;
    if parser_token_count() > 0 {
        first_type = tok_type(get_token(0));
    }

    if is_decl_token(first_type) {
        // Parse as program (declarations)
        var prog *u8 = parse_program();
        if parse_error_count > 0 { return nil; }
        return ast_emit_program(prog);
    } else {
        // Parse as expression
        var expr *u8 = parse_expression();
        if parse_error_count > 0 { return nil; }
        return ast_emit_expression(expr);
    }
}

